{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2643a9",
   "metadata": {},
   "source": [
    "**Having performed a thorough exploratory data analysis, the roadmap for preprocessing the data is clear.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84188aec",
   "metadata": {},
   "source": [
    "## Required Preprocessing Steps:\n",
    "1. [Load raw data.](#data-loading)\n",
    "\n",
    "2. [Normalize `[\"recipe_title\", \"description\", \"ingredients\", \"directions\"]` fields for consistent formating.](#normalization)\n",
    "\n",
    "3. [Delete duolicates based on `[\"recipe_title\", \"description\", \"ingredients\", \"directions\"]` combination.](#duplicate-deletion)\n",
    "\n",
    "4. [Extract `[\"recipe_title\", \"ingredients\", \"directions\"]` columns only.](#feature-extraction)\n",
    "\n",
    "5. [Clean ingredients using *stage 1 and stage 2* functions.](#2-stage-ingredient-cleaning)\n",
    "\n",
    "6. [Final Cleaning of Ingredients.](#final-cleaning)\n",
    "\n",
    "7. [Build TF-IDF matrix with canonicalized ingredients](#tf-idf)\n",
    "\n",
    "8. [Compute cosine similarity](#similarity-computation)\n",
    "\n",
    "9. [Embeddings (Semantic Matching).](#embeddings-semantic-matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a66c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import hdbscan\n",
    "import pickle\n",
    "import re\n",
    "import ast\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset\n",
    "from itertools import combinations\n",
    "from functools import reduce\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from rapidfuzz import process, fuzz  # faster fuzzy matching\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388478f",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd47093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading raw data\n",
    "# Loading dataset\n",
    "raw_data = load_dataset(\"json\", data_files=\"../data/recipe.json\")\n",
    "df = raw_data[\"train\"].to_pandas()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0e8cf",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring consistency in string formatting\n",
    "def normalize_column(col):\n",
    "    \"\"\"\n",
    "    Normalize a column for duplicate detection:\n",
    "    - Strings → lowercase, stripped\n",
    "    - Lists/arrays → lowercase, stripped, sorted, converted to tuple\n",
    "    \"\"\"\n",
    "    if isinstance(col, str):\n",
    "        return col.strip().lower()\n",
    "    elif isinstance(col, (list, np.ndarray)):\n",
    "        # Lowercase each element, strip spaces, sort, convert to tuple\n",
    "        cleaned = tuple(sorted([str(x).strip().lower() for x in col]))\n",
    "        return cleaned\n",
    "    return col\n",
    "\n",
    "# Columns to check for duplicates\n",
    "cols_to_check = [\"recipe_title\", \"description\", \"ingredients\", \"directions\"]\n",
    "\n",
    "# Create normalized columns\n",
    "normalized_cols = {col: df[col].apply(normalize_column) for col in cols_to_check}\n",
    "\n",
    "# Combine into a DataFrame\n",
    "norm_df = df.assign(**normalized_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a903a14",
   "metadata": {},
   "source": [
    "### Duplicate Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting duplicates\n",
    "deduplicated_df = norm_df.drop_duplicates(subset=[\"recipe_title\", \"description\", \"ingredients\", \"directions\"])\n",
    "deduplicated_df = deduplicated_df.copy()\n",
    "deduplicated_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c246c31",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting relevant fields\n",
    "recipes_df = deduplicated_df[[\"recipe_title\", \"ingredients\", \"directions\"]]\n",
    "recipes_df = recipes_df.copy()\n",
    "recipes_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fef70b",
   "metadata": {},
   "source": [
    "### 2-Stage Ingredient-Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9c02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medium language model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff29174a",
   "metadata": {},
   "source": [
    "#### Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stage1(raw):\n",
    "    raw = raw.lower().strip()\n",
    "    \n",
    "    # Replace unicode fractions\n",
    "    UNICODE_FRAC = {\"½\": \"1/2\", \"⅓\": \"1/3\", \"¼\": \"1/4\", \"⅛\": \"1/8\", \"⅔\": \"2/3\", \"¾\" : \"3/4\", \"⅜\" : \"3/8\", \"⅞\" : \"7/8\"}\n",
    "    for frac, val in UNICODE_FRAC.items():\n",
    "        raw = raw.replace(frac, val)\n",
    "        \n",
    "    # Remove “such as ...” example clauses\n",
    "    raw = re.sub(r\"such as [a-zA-Z\\s']+\", \" \", raw)\n",
    "    \n",
    "    # Remove numbers but keep B12/B6 etc.\n",
    "    raw = re.sub(r'\\b\\d+(\\.\\d+)?\\b(?![a-zA-Z])', ' ', raw)\n",
    "\n",
    "    # Remove numeric quantities\n",
    "    # raw = re.sub(r\"(\\d+\\/\\d+|\\d+\\.\\d+|\\d+)\", \" \", raw)\n",
    "\n",
    "    # Remove measurement units\n",
    "    raw = re.sub(r\"\\b(cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|gram|grams|kg|kilogram|kilograms|pound|pounds|lb|\\\n",
    "                 pinch|pint|quart|quartered|dash|sprig|inch|inches|pieces|sized|size|whole)\\b\", \" \", raw)\n",
    "\n",
    "    # Remove preparation-only words (but do not remove meaningful adjectives)\n",
    "    PREP_WORDS = r\"(chopped|diced|minced|sliced|slices|skinned|peeled|halved|shucked|shredded|ground|grated|granulated|trimmed|rinsed|patted|divided|optional|crush|crushed|garnish|\\\n",
    "        cooked|prepared|cut|pat|dry|thaw|drained|refrigerate|frozen|thawed|remove|dusting|squeezed|scrubbed|finely|coarse|coarsely|cold|unsalted|lightly|crumbled|thick|processed)\"\n",
    "    raw = re.sub(rf\"\\b{PREP_WORDS}\\b\", \" \", raw)\n",
    "\n",
    "    # Remove other terms\n",
    "    OTHER_WORDS = r\"(plus|more|into|for|extra|additional|taste|package|bag|box|can|cans|canned|tube|jar|bottle|container|about|total|desired|needed|serving|note|icing|dipping|wooden|\\\n",
    "        toothpicks|skewers|parchment|packet|baby|everything|italian-style|japanese-style|american)\"\n",
    "    raw = re.sub(rf\"\\b{OTHER_WORDS}\\b\", \" \", raw)\n",
    "\n",
    "    # Remove punctuation while leaving some possible ingredient connectors\n",
    "    raw = re.sub(r\"[^\\w\\s/,&-]\", \" \", raw)\n",
    "\n",
    "    # Collapse whitespace\n",
    "    raw = re.sub(r\"\\s+\", \" \", raw).strip()\n",
    "    \n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87efbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataset\n",
    "recipes_df[\"clean_ingredients_stage1\"] = recipes_df[\"ingredients\"].apply(\n",
    "    lambda lst: [clean_stage1(raw) for raw in lst]\n",
    ")\n",
    "\n",
    "# Preview\n",
    "recipes_df[['ingredients', 'clean_ingredients_stage1']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e5de0",
   "metadata": {},
   "source": [
    "#### Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens(text):\n",
    "    \"\"\"\n",
    "    Split Stage-1 cleaned text into tokens.\n",
    "    Uses commas, slashes, ' and ', ' or ' etc.\n",
    "    \"\"\"\n",
    "    # replace and/or with comma (but not inside ingredient names)\n",
    "    text = re.sub(r\"\\s+(and|or|\\&)\\s+\", \",\", text)\n",
    "\n",
    "    # split on commas or slashes\n",
    "    raw_tokens = re.split(r\"[,/]\", text)\n",
    "\n",
    "    # clean whitespace\n",
    "    return [t.strip() for t in raw_tokens if t.strip()]\n",
    "\n",
    "\n",
    "def looks_like_garbage(token):\n",
    "    \"\"\"\n",
    "    Shape-based garbage detection.\n",
    "    No vocabulary lists — entirely rule-based.\n",
    "    \"\"\"\n",
    "    t = token.lower().strip()\n",
    "\n",
    "    # too short (except valid short ingredients)\n",
    "    if len(t) <= 2 and t not in {\"oil\", \"yam\", \"tea\"}:\n",
    "        return True\n",
    "\n",
    "    # remove tokens ending in filler words\n",
    "    if re.search(r\"(needed|serving|taste|note)$\", t):\n",
    "        return True\n",
    "\n",
    "    # remove repeated nonsense like \"wet wet sauce\"\n",
    "    if re.search(r\"\\b(\\w+)\\s+\\1\\b\", t):\n",
    "        return True\n",
    "\n",
    "    # no alphabetic characters\n",
    "    if not re.search(r\"[a-zA-Z]\", t):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def extract_ingredient_phrase(t):\n",
    "    \"\"\"\n",
    "    Extracts the main ingredient phrase using POS-based noun chunking,\n",
    "    preserving multiword ingredients naturally.\n",
    "    \"\"\"\n",
    "    t = t.strip().lower()\n",
    "\n",
    "    # Salt-and-pepper pattern\n",
    "    if \" and \" in t:\n",
    "        parts = [extract_ingredient_phrase(x) for x in t.split(\" and \")]\n",
    "        flat = []\n",
    "        for p in parts:\n",
    "            if isinstance(p, list):\n",
    "                flat.extend(p)\n",
    "            else:\n",
    "                flat.append(p)\n",
    "        return flat\n",
    "\n",
    "    doc = nlp(t)\n",
    "\n",
    "    # POS-based chunks (noun phrases)\n",
    "    noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "    if noun_chunks:\n",
    "        phrase = noun_chunks[-1]  # get the main noun phrase\n",
    "    else:\n",
    "        phrase = t\n",
    "\n",
    "    # Remove undesirable descriptors but keep food words\n",
    "    DESCRIPTORS = {\n",
    "        \"large\", \"small\", \"fresh\", \"freshly\", \"boneless\", \"skinless\", \"zested\", \"juiced\", \"minced\", \"toasted\", \"cooked\", \"flaked\", \"unsweetened\", \"roasted\",\n",
    "        \"4ounce\", \"bone-in\", \"skin\", \"round\", \"salted\", \"uncooked\", \"seasoned\", \"ground\", \"crushed\", \"sliced\", \"diced\", \"creamy\", \"halved\", \"beaten\",\n",
    "        \"melted\", \"softened\", \"cooked\", \"split\", \"nugget\", \"dried\", \"s\", \"lbs\", \"-\", \"-half\", \"ablespoons\", \"nonstick\", \"cooking\", \"spray\"\n",
    "    }\n",
    "\n",
    "    words = []\n",
    "    for w in phrase.split():\n",
    "        if w not in DESCRIPTORS:\n",
    "            words.append(w)\n",
    "\n",
    "    cleaned = \" \".join(words).strip()\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def remove_filler_words(phrase):\n",
    "    \"\"\"\n",
    "    Removes standalone filler words from final ingredient tokens,\n",
    "    but does NOT destroy valid multiword ingredient names.\n",
    "    \"\"\"\n",
    "    FILLER_STOPWORDS = {\n",
    "        \"and\", \"to\", \"or\", \"for\", \"with\", \"in\", \"of\", \"the\",\n",
    "        \"a\", \"an\", \"as\", \"on\", \"into\", \"at\"\n",
    "        }\n",
    "    words = phrase.split()\n",
    "    words = [w for w in words if w not in FILLER_STOPWORDS]\n",
    "    return \" \".join(words).strip()\n",
    "\n",
    "\n",
    "def clean_stage2(stage1_output):\n",
    "    tokens = split_tokens(stage1_output)\n",
    "\n",
    "    cleaned = []\n",
    "\n",
    "    for t in tokens:\n",
    "        t = t.strip().lower()\n",
    "        \n",
    "        # remove leading/trailing punctuation\n",
    "        t = re.sub(r\"^[^\\w]+|[^\\w]+$\", \"\", t)\n",
    "\n",
    "        # Skip garbage tokens\n",
    "        if looks_like_garbage(t):\n",
    "            continue\n",
    "\n",
    "        # Extract ingredient phrase\n",
    "        extracted = extract_ingredient_phrase(t)\n",
    "\n",
    "        # handle salt-and-pepper cases (list return)\n",
    "        if isinstance(extracted, list):\n",
    "            for x in extracted:\n",
    "                x = remove_filler_words(x)\n",
    "                if x and not looks_like_garbage(x):\n",
    "                    cleaned.append(x)\n",
    "        else:\n",
    "            x = remove_filler_words(extracted)\n",
    "            if x and not looks_like_garbage(x):\n",
    "                cleaned.append(x)\n",
    "\n",
    "    # Remove duplicates while maintaining order\n",
    "    final = list(dict.fromkeys(cleaned))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to actual dataset\n",
    "recipes_df[\"clean_ingredients_stage2\"] = recipes_df[\"clean_ingredients_stage1\"].apply(\n",
    "    lambda lst: [clean_stage2(raw) for raw in lst]\n",
    ")\n",
    "\n",
    "# Preview\n",
    "recipes_df[['clean_ingredients_stage1', 'clean_ingredients_stage2']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f8966",
   "metadata": {},
   "source": [
    "### Flatten Nested Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fdd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten clean_ingredients_stage2 output one level: [[...], [...]] → [...]\n",
    "def parse_ingredients(x):\n",
    "    if isinstance(x, str):\n",
    "        x = ast.literal_eval(x)\n",
    "    return [item for sublist in x for item in sublist]\n",
    "\n",
    "recipes_df[\"clean_ingredients\"] = (\n",
    "    recipes_df[\"clean_ingredients_stage2\"]\n",
    "    .apply(parse_ingredients)\n",
    ")\n",
    "\n",
    "recipes_df['clean_ingredients'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f8402",
   "metadata": {},
   "source": [
    "### Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa29529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cleaning(ingredients):\n",
    "    \"\"\"\n",
    "    ingredients: List[str]\n",
    "    returns: List[str]\n",
    "    \"\"\"\n",
    "    if not ingredients:\n",
    "        return []\n",
    "    \n",
    "    cleaned = []\n",
    "    \n",
    "    # Remove undesirable descriptors\n",
    "    UNWANTED_WORDS = {\n",
    "        \"large\", \"small\", \"fresh\", \"freshly\", \"boneless\", \"skinless\", \"zested\", \"juiced\", \"minced\", \"toasted\", \"cooked\", \"flaked\", \"unsweetened\", \"roasted\",\n",
    "        \"4ounce\", \"bone-in\", \"skin\", \"round\", \"salted\", \"uncooked\", \"seasoned\", \"ground\", \"crushed\", \"sliced\", \"diced\", \"creamy\", \"halved\", \"beaten\",\n",
    "        \"melted\", \"softened\", \"cooked\", \"split\", \"nugget\", \"dried\", \"s\", \"lbs\", \"-\", \"half\", \"ablespoons\", \"nonstick\", \"cooking\", \"spray\", \"all\", \"purpose\",\n",
    "        \"2tablespoons\", \"4cup\", \"3x1\", \"added\", \"white\", \"brown\", \"red\", \"green\", \"black\", \"yellow\", \"undrained\", \"aluminium\", \"foil\", \"packaged\", \"reduced\", \"medium\", \"sodium\", \n",
    "        \"pure\", \"stemmed\", \"color\", \"flavor\", \"allpurpose\", \"almondflavored\", \"work\", \"surface\", \"very\", \"hot\", \"soft\", \"thin\", \"thick\", \"bunch\", \"plain\", \"italian\", \"glutenfree\",\n",
    "        \"sweet\", \"sugarbased\", \"sugarfree\", \"semisweet\", \"seeded\", \"seedless\"\n",
    "        }\n",
    "    \n",
    "    for ing in ingredients:\n",
    "\n",
    "        # remove punctuation\n",
    "        ing = re.sub(r'[^\\w\\s]', '', ing)\n",
    "        \n",
    "        tokens = [\n",
    "            t for t in ing.split() if t not in UNWANTED_WORDS\n",
    "        ]\n",
    "        if tokens:\n",
    "            cleaned.append(\" \".join(tokens))\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40496cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataset\n",
    "recipes_df[\"clean_ingredients\"] = recipes_df[\"clean_ingredients\"].apply(final_cleaning)\n",
    "\n",
    "# Preview\n",
    "recipes_df['clean_ingredients'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a278565",
   "metadata": {},
   "source": [
    "### Singularize Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b162b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_normalize(ingredients, nlp):\n",
    "    normalized = []\n",
    "\n",
    "    for ing in ingredients:\n",
    "        doc = nlp(ing)\n",
    "        words = []\n",
    "        for t in doc:\n",
    "            if t.pos_ == \"NOUN\":\n",
    "                words.append(t.lemma_)\n",
    "            else:\n",
    "                words.append(t.text)\n",
    "        normalized.append(\" \".join(words))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41924a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_df[\"clean_ingredients_norm\"] = (\n",
    "    recipes_df[\"clean_ingredients\"]\n",
    "    .apply(lambda x: light_normalize(x, nlp))\n",
    ")\n",
    "\n",
    "recipes_df['clean_ingredients_norm'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd04412",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6cbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = recipes_df['clean_ingredients_norm'].apply(\"|\".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06177abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 1),      # capture \"soy sauce\", \"fish sauce\"\n",
    "    min_df=3,                # drop very rare noise\n",
    "    max_df=0.85,             # suppress salt/oil/water\n",
    "    norm=\"l2\",               # cosine similarity friendly\n",
    "    use_idf=True,\n",
    "    token_pattern=r\"[^|]+\",\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True        # log(1 + tf)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26bdc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "idf = vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (very rare ingredients)\n",
    "top = sorted(zip(feature_names, idf), key=lambda x: -x[1])[:1000]\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed84b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least informative (very commom ingrdients)\n",
    "bottom = sorted(zip(feature_names, idf), key=lambda x: x[1])[:20]\n",
    "bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891cc74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4d742",
   "metadata": {},
   "source": [
    "### Similarity Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4375d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the TF-IDF sparse matrix\n",
    "X_sparse = csr_matrix(X)\n",
    "\n",
    "# Normalize for cosine similarity (makes it dot product)\n",
    "X_norm = normalize(X_sparse, norm='l2', axis=1)\n",
    "\n",
    "# Convert ingredient lists to sets\n",
    "# ingredient_sets = recipes_df['clean_ingredients_norm'].apply(set).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e090c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix\n",
    "similarity_matrix = cosine_similarity(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.savez_compressed(\"similarity_matrix.npz\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9da863",
   "metadata": {},
   "source": [
    "### Embeddings (Semantic Matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a85223",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "ingredient_texts = recipes_df[\"clean_ingredients_norm\"].apply(\n",
    "    lambda xs: \", \".join(xs)\n",
    ").tolist()\n",
    "\n",
    "embeddings = model.encode(\n",
    "    ingredient_texts,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de90a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save(\"recipe_embeddings.npy\", embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
